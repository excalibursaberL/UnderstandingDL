{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过 Log-Sum-Exp 逼近最大值实现最大池化",
   "id": "3b6189969ef45f95"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T14:07:59.734253Z",
     "start_time": "2025-12-22T14:07:58.257570Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftMaxPoolAsConv(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, stride=2, beta=10):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "        # 定义卷积层用于“求和”\n",
    "        # 这里的设置和平均池化的一样：Groups=Channels，Bias=False\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            groups=channels,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # 将卷积核权重初始化为 1.0\n",
    "        # 因为我们要计算的是 sum(e^x)，而不是 average(e^x)\n",
    "        self.conv.weight.data.fill_(1.0)\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 步骤 1: 放大并指数化 (非线性部分)\n",
    "        # e^(beta * x)\n",
    "        x_exp = torch.exp(self.beta * x)\n",
    "\n",
    "        # 步骤 2: 使用卷积层进行求和 (线性部分)\n",
    "        # sum(...)\n",
    "        x_sum = self.conv(x_exp)\n",
    "\n",
    "        # 步骤 3: 取对数并缩小 (还原部分)\n",
    "        # (1/beta) * ln(...)\n",
    "        # 加一个极小值 1e-8 防止 log(0)\n",
    "        output = (1 / self.beta) * torch.log(x_sum + 1e-8)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "验证",
   "id": "2fac93cf5b3696bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T14:08:02.187252Z",
     "start_time": "2025-12-22T14:08:02.168898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 验证测试 ---\n",
    "\n",
    "# 创建简单数据\n",
    "X = torch.tensor([\n",
    "    [\n",
    "        [1.0, 5.0],\n",
    "        [2.0, 8.0]\n",
    "    ]\n",
    "]).unsqueeze(0) # (1, 1, 2, 2)\n",
    "\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "\n",
    "# 1. 真实的最大池化\n",
    "true_max = F.max_pool2d(X, kernel_size=2)\n",
    "print(f\"\\n真实 Max Pooling 结果: {true_max.item()}\")\n",
    "\n",
    "# 2. 使用卷积逼近 (Beta 越大越精确，但容易溢出)\n",
    "# 尝试不同的 Beta 值\n",
    "for b in [1, 10, 50]:\n",
    "    approx_layer = SoftMaxPoolAsConv(channels=1, kernel_size=2, stride=2, beta=b)\n",
    "    approx_res = approx_layer(X)\n",
    "    print(f\"卷积逼近结果 (beta={b}): {approx_res.item():.4f}\")\n"
   ],
   "id": "9de5fdc5c8a0f853",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "tensor([[[[1., 5.],\n",
      "          [2., 8.]]]])\n",
      "\n",
      "真实 Max Pooling 结果: 8.0\n",
      "卷积逼近结果 (beta=1): 8.0518\n",
      "卷积逼近结果 (beta=10): 8.0000\n",
      "卷积逼近结果 (beta=50): inf\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
