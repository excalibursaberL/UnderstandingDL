{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7be7b6bf2e1eeb2",
   "metadata": {},
   "source": [
    "使用 cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set model cache directory (must be set before importing torch)\n",
    "MODELS_DIR = '../models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.environ['TORCH_HOME'] = MODELS_DIR\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model cache directory: {os.path.abspath(MODELS_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35b65b5849201a",
   "metadata": {},
   "source": [
    "定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = '../data/ClassifyLeaves'\n",
    "BATCH_SIZE = 32  # Reduced for ResNet50 (larger model)\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "VAL_RATIO = 0.2  # 20% for validation\n",
    "NUM_WORKERS = 0  # Set to 0 for Windows compatibility, can increase on Linux\n",
    "MODEL_SAVE_PATH = os.path.join(MODELS_DIR, 'kcf-resnet50.pth')\n",
    "\n",
    "# Image size for ResNet\n",
    "IMG_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58bc1bcd145e31",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc87381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv and create label mapping\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "\n",
    "# Create label to index mapping\n",
    "labels = sorted(train_df['label'].unique())\n",
    "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff982a5f134ec6",
   "metadata": {},
   "source": [
    "定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08813404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class\n",
    "class LeavesDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, label2idx=None, transform=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.label2idx = label2idx\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.df.loc[idx, 'image'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            label = self.df.loc[idx, 'label']\n",
    "            label_idx = self.label2idx[label]\n",
    "            return image, label_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a38b2599514204",
   "metadata": {},
   "source": [
    "数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation and transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49de4ce3367b39",
   "metadata": {},
   "source": [
    "划分训练集与验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=VAL_RATIO, \n",
    "    random_state=42, \n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LeavesDataset(train_data, DATA_DIR, label2idx, transform=train_transform)\n",
    "val_dataset = LeavesDataset(val_data, DATA_DIR, label2idx, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3787a11a03ed0",
   "metadata": {},
   "source": [
    "定义 ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - ResNet50 with pretrained weights\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the last fully connected layer for our number of classes\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a76ea895de2975",
   "metadata": {},
   "source": [
    "定义训练与验证操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb622560190b2e",
   "metadata": {},
   "source": [
    "进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'label2idx': label2idx,\n",
    "            'idx2label': idx2label\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"  -> Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f0f1550f36ecd",
   "metadata": {},
   "source": [
    "可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "# Plot training loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation loss\n",
    "axes[1].plot(epochs_range, history['val_loss'], 'r-', linewidth=2, label='Val Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation accuracy\n",
    "axes[2].plot(epochs_range, history['val_acc'], 'g-', linewidth=2, label='Val Accuracy')\n",
    "axes[2].axhline(y=best_val_acc, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_val_acc:.2f}%')\n",
    "axes[2].scatter([best_epoch], [best_val_acc], color='r', s=100, zorder=5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Validation Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining history saved to 'training_history.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e6b662b8a96f4",
   "metadata": {},
   "source": [
    "可视化训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f24f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined loss plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', linewidth=2, label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Train vs Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'g-', linewidth=2, label='Val Accuracy')\n",
    "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_val_acc:.2f}%')\n",
    "axes[1].scatter([best_epoch], [best_val_acc], color='r', s=100, zorder=5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d61bee9199011",
   "metadata": {},
   "source": [
    "进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ccc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for prediction\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "idx2label = checkpoint['idx2label']\n",
    "print(f\"Loaded best model from: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = LeavesDataset(test_df, DATA_DIR, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db250212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc='Predicting')\n",
    "    for images in pbar:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert indices to labels\n",
    "predicted_labels = [idx2label[idx] for idx in predictions]\n",
    "\n",
    "print(f\"Total predictions: {len(predicted_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c99c905eb9a8d",
   "metadata": {},
   "source": [
    "将预测结果写入 sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'image': test_df['image'],\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save to submission file\n",
    "submission_path = os.path.join(DATA_DIR, 'sample_submission.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nLabel distribution (top 10):\")\n",
    "print(submission_df['label'].value_counts().head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
